{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udd2c Unsupervised Learning \u2014 Clustering Algorithms\n\n**Course:** CSC582 \u2014 Data Warehousing and Mining | King Saud University\n\n**Reference:** Introduction to Machine Learning with Python \u2014 Chapter 3 (pp. 168\u2013207)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import make_blobs, make_moons, load_iris\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics.cluster import adjusted_rand_score, silhouette_score\nfrom sklearn.metrics import accuracy_score, pairwise_distances_argmin\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster.hierarchy import dendrogram, linkage, ward\n\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 12\n\nprint('All imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 1. k-Means Clustering (pp. 168\u2013181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Manual k-Means with Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\nprint(\"1.1 Manual k-Means with Decision Boundaries\")\nprint(\"=\" * 65)\n\n# Generate Synthetic Data (300 samples, 4 natural clusters)\nX_km, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\nk_value = 4  # number of clusters\n\n# Initialize: pick random points as starting centers\nrng = np.random.RandomState(42)\ninitial_indices = rng.permutation(X_km.shape[0])[:k_value]\ncenters = X_km[initial_indices]\n\n\n# Function to draw the decision boundaries (Voronoi regions)\ndef plot_decision_boundaries(ax, centers, X):\n    h = 0.02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Calculate Euclidean distance for every point in the background grid\n    Z = pairwise_distances_argmin(np.c_[xx.ravel(), yy.ravel()], centers)\n    Z = Z.reshape(xx.shape)\n\n    # Plot the color-coded regions\n    ax.imshow(Z, interpolation='nearest',\n              extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n              cmap='Pastel1', aspect='auto', origin='lower', alpha=0.4)\n\n\n# Run k-Means manually, step by step\niteration = 0\nhistory = []  # store (centers, labels) at each step\n\nwhile True:\n    iteration += 1\n    # Assignment Step: assign each point to nearest center (Euclidean distance)\n    labels = pairwise_distances_argmin(X_km, centers)\n    history.append((centers.copy(), labels.copy()))\n\n    # Update Step: move centers to the mean of their assigned points\n    new_centers = np.array([X_km[labels == j].mean(0) for j in range(k_value)])\n\n    # Check for convergence\n    if np.all(centers == new_centers):\n        print(f\"Converged after {iteration} iterations!\")\n        break\n    centers = new_centers\n\n# Show iterations: first, middle, and final\nshow_iters = [0, len(history) // 2, len(history) - 1]\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor ax, idx in zip(axes, show_iters):\n    c, l = history[idx]\n    plot_decision_boundaries(ax, c, X_km)\n    ax.scatter(X_km[:, 0], X_km[:, 1], c=l, s=40, cmap='viridis', edgecolors='k', linewidth=0.3)\n    ax.scatter(c[:, 0], c[:, 1], c='red', s=250, marker='^', edgecolors='black',\n               linewidth=1.5, label='Centers', zorder=5)\n    ax.set_title(f'Iteration {idx + 1}', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Feature 0')\n    ax.set_ylabel('Feature 1')\n    ax.legend(fontsize=10)\n\nplt.suptitle(f'Manual k-Means Step-by-Step (k={k_value}, converged in {len(history)} iterations)',\n             fontsize=16, fontweight='bold', y=1.03)\nplt.tight_layout()\nplt.savefig('plot_1_1_kmeans_manual.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Final result with full decision boundaries\nfig, ax = plt.subplots(figsize=(10, 8))\nfinal_centers, final_labels = history[-1]\nplot_decision_boundaries(ax, final_centers, X_km)\nax.scatter(X_km[:, 0], X_km[:, 1], c=final_labels, s=50, cmap='viridis', edgecolors='k')\nax.scatter(final_centers[:, 0], final_centers[:, 1], c='red', s=250, marker='^',\n           edgecolors='black', linewidth=1.5, label='Cluster Centers')\nax.set_title(f'Final Decision Boundaries (k={k_value})\\nTotal Iterations: {len(history)}',\n             fontsize=14, fontweight='bold')\nax.set_xlabel('Feature 0')\nax.set_ylabel('Feature 1')\nax.legend(fontsize=12)\nplt.tight_layout()\nplt.savefig('plot_1_1_kmeans_final.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Elbow Method \u2014 Choosing Optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n1.2 Elbow Method \u2014 Choosing Optimal k\")\n\nX_elbow, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\ninertias = []\nsil_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=0, n_init=10)\n    labels = km.fit_predict(X_elbow)\n    inertias.append(km.inertia_)\n    sil_scores.append(silhouette_score(X_elbow, labels))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\nax1.set_title('Elbow Method \u2014 Inertia', fontsize=14, fontweight='bold')\nax1.set_xlabel('Number of Clusters (k)')\nax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)')\nax1.axvline(x=4, color='red', linestyle='--', label='Elbow at k=4')\nax1.legend()\n\nax2.plot(K_range, sil_scores, 'go-', linewidth=2, markersize=8)\nax2.set_title('Silhouette Score vs k', fontsize=14, fontweight='bold')\nax2.set_xlabel('Number of Clusters (k)')\nax2.set_ylabel('Silhouette Score')\nax2.axvline(x=4, color='red', linestyle='--', label='Best at k=4')\nax2.legend()\n\nplt.suptitle('How to Choose the Best k', fontsize=16, fontweight='bold', y=1.03)\nplt.tight_layout()\nplt.savefig('plot_1_2_elbow.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'Best silhouette score: {max(sil_scores):.3f} at k={list(K_range)[np.argmax(sil_scores)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 k-Means Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n1.3 k-Means Failure Cases\")\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Failure 1: Different densities\nX_dense, y_dense = make_blobs(n_samples=300, cluster_std=[1.0, 2.5, 0.5], random_state=170)\nlabels_dense = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_dense)\naxes[0].scatter(X_dense[:, 0], X_dense[:, 1], c=labels_dense, cmap='viridis', s=40, edgecolors='k', linewidth=0.3)\naxes[0].set_title('Different Densities\\n(misassigns sparse cluster)', fontsize=13, fontweight='bold')\n\n# Failure 2: Elongated/non-spherical\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(make_blobs(n_samples=300, random_state=170)[0], transformation)\nlabels_aniso = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_aniso)\naxes[1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=labels_aniso, cmap='viridis', s=40, edgecolors='k', linewidth=0.3)\naxes[1].set_title('Elongated Clusters\\n(cuts through natural groups)', fontsize=13, fontweight='bold')\n\n# Failure 3: Two moons\nX_moons_fail, y_moons_fail = make_moons(n_samples=200, noise=0.05, random_state=0)\nlabels_moons_fail = KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X_moons_fail)\naxes[2].scatter(X_moons_fail[:, 0], X_moons_fail[:, 1], c=labels_moons_fail, cmap='viridis', s=40, edgecolors='k', linewidth=0.3)\naxes[2].set_title('Two Moons\\n(spherical assumption fails)', fontsize=13, fontweight='bold')\n\nfor ax in axes:\n    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n\nplt.suptitle('k-Means Failure Cases \u2014 When Spherical Assumption Breaks',\n             fontsize=16, fontweight='bold', y=1.03)\nplt.tight_layout()\nplt.savefig('plot_1_3_kmeans_failures.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 2. Agglomerative Clustering (pp. 182\u2013187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 65)\nprint(\"2.1 Basic Agglomerative Clustering\")\nprint(\"=\" * 65)\n\nX_agg, y_agg = make_blobs(n_samples=8, centers=3, random_state=1)\n\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels_agg = agg.fit_predict(X_agg)\n\nplt.figure(figsize=(6, 5))\nplt.scatter(X_agg[:, 0], X_agg[:, 1], c=labels_agg, s=100, cmap='viridis',\n            edgecolors='k', linewidth=0.5)\n\n# Add point labels\nfor i, (x, y_coord) in enumerate(X_agg):\n    plt.text(x + 0.1, y_coord + 0.1, str(i), fontsize=12, fontweight='bold')\n\nplt.title(\"Agglomerative Clustering (3 Clusters) with Point Labels\",\n          fontsize=14, fontweight='bold')\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.tight_layout()\nplt.savefig('plot_2_1_agglomerative.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'Cluster labels: {labels_agg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dendrogram (Ward Linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2.2 Dendrogram (Ward Linkage)\")\n\nlinked = linkage(X_agg, method='ward')\n\nplt.figure(figsize=(8, 5))\ndendrogram(linked)\nplt.title(\"Dendrogram (Ward Linkage)\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Data Points\")\nplt.ylabel(\"Distance\")\nplt.tight_layout()\nplt.savefig('plot_2_2_dendrogram.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Linkage Comparison (Ward vs Complete vs Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 2.3 Agglomerative on Larger Dataset + Linkage Comparison \u2500\u2500\nprint(\"\\n2.3 Linkage Comparison (Ward vs Complete vs Average)\")\n\n# Use ELONGATED (anisotropic) blobs \u2014 linkage choice matters here!\n# With well-separated round blobs all linkages give identical results.\nX_aniso_raw, _ = make_blobs(n_samples=150, centers=3, random_state=170)\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X_aniso_raw, transformation)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nlinkage_types = ['ward', 'complete', 'average']\n\nfor ax, link_type in zip(axes, linkage_types):\n    agg = AgglomerativeClustering(n_clusters=3, linkage=link_type)\n    labels = agg.fit_predict(X_aniso)\n    ax.scatter(X_aniso[:, 0], X_aniso[:, 1], c=labels, s=50, cmap='viridis',\n               edgecolors='k', linewidth=0.3)\n    ax.set_title(f'Linkage: {link_type.capitalize()}', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\nplt.suptitle('Agglomerative Clustering \u2014 Linkage Comparison on Elongated Data (k=3)\\n'\n             'Notice: each linkage draws different boundaries!',\n             fontsize=16, fontweight='bold', y=1.05)\nplt.tight_layout()\nplt.savefig('plot_2_3_linkage_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 3. DBSCAN Clustering (pp. 187\u2013190)\n\n**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 DBSCAN Parameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 65)\nprint(\"3.1 DBSCAN Parameter Exploration\")\nprint(\"=\" * 65)\n\nX_db, y_db = make_blobs(random_state=0, n_samples=12)\n\nprint(f'{\"min_samples\":>12} {\"eps\":>6} {\"clusters\":>40}')\nprint('-' * 62)\n\nfor min_s in [2, 3, 5]:\n    for eps_val in [1.0, 1.5, 2.0, 3.0]:\n        db = DBSCAN(min_samples=min_s, eps=eps_val)\n        clusters = db.fit_predict(X_db)\n        print(f'{min_s:>12} {eps_val:>6.1f} {str(clusters):>40}')\n\nprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Effect of `eps` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2 Effect of eps Parameter\")\nX_moons, y_moons = make_moons(n_samples=300, noise=0.06, random_state=0)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_moons)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 11))\neps_values = [0.1, 0.2, 0.3, 0.5, 0.8, 1.5]\n\nfor ax, eps_val in zip(axes.ravel(), eps_values):\n    db = DBSCAN(eps=eps_val, min_samples=5)\n    labels = db.fit_predict(X_scaled)\n\n    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise = list(labels).count(-1)\n\n    noise_mask = labels == -1\n    ax.scatter(X_scaled[noise_mask, 0], X_scaled[noise_mask, 1],\n               c='red', marker='x', s=50, label=f'Noise ({n_noise} pts)', zorder=3)\n\n    cluster_mask = labels != -1\n    if cluster_mask.any():\n        ax.scatter(X_scaled[cluster_mask, 0], X_scaled[cluster_mask, 1],\n                   c=labels[cluster_mask], cmap='viridis', s=40,\n                   edgecolors='black', linewidth=0.3)\n\n    if eps_val <= 0.5:\n        circle = plt.Circle((X_scaled[150, 0], X_scaled[150, 1]),\n                             eps_val, fill=False, color='red', linewidth=2,\n                             linestyle='--', alpha=0.7)\n        ax.add_patch(circle)\n        ax.plot(X_scaled[150, 0], X_scaled[150, 1], 'r*', markersize=15, zorder=5)\n\n    if n_clusters == 0: result, color = 'ALL NOISE!', 'red'\n    elif n_clusters == 2: result, color = 'CORRECT!', 'green'\n    elif n_clusters == 1: result, color = 'One big cluster', 'orange'\n    else: result, color = 'Too fragmented', 'orange'\n\n    ax.set_title(f'eps = {eps_val}\\n{n_clusters} clusters, {n_noise} noise \u2014 {result}',\n                 fontsize=13, fontweight='bold', color=color)\n    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n    ax.legend(loc='upper right', fontsize=9)\n\nfig.suptitle('EFFECT OF eps (min_samples fixed at 5)\\n'\n             'Red dashed circle = eps neighborhood radius',\n             fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig('plot_3_2_eps_effect.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Effect of `min_samples` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3 Effect of min_samples Parameter\")\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 11))\nmin_samples_values = [2, 3, 5, 10, 20, 50]\n\nfor ax, min_s in zip(axes.ravel(), min_samples_values):\n    db = DBSCAN(eps=0.5, min_samples=min_s)\n    labels = db.fit_predict(X_scaled)\n\n    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise = list(labels).count(-1)\n\n    core_mask = np.zeros(len(labels), dtype=bool)\n    if hasattr(db, 'core_sample_indices_') and len(db.core_sample_indices_) > 0:\n        core_mask[db.core_sample_indices_] = True\n\n    noise_mask = labels == -1\n    border_mask = (labels != -1) & (~core_mask)\n\n    ax.scatter(X_scaled[noise_mask, 0], X_scaled[noise_mask, 1],\n               c='red', marker='x', s=50, label=f'Noise ({n_noise})', zorder=3)\n    if border_mask.any():\n        ax.scatter(X_scaled[border_mask, 0], X_scaled[border_mask, 1],\n                   c=labels[border_mask], cmap='viridis', s=30,\n                   edgecolors='black', linewidth=0.3, alpha=0.6)\n    if core_mask.any():\n        ax.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n                   c=labels[core_mask], cmap='viridis', s=60,\n                   edgecolors='black', linewidth=0.5)\n\n    n_core = core_mask.sum()\n    n_border = border_mask.sum()\n    ax.set_title(f'min_samples = {min_s}\\n{n_clusters} clusters | '\n                 f'Core: {n_core} | Border: {n_border} | Noise: {n_noise}',\n                 fontsize=12, fontweight='bold')\n    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n    ax.legend(loc='upper right', fontsize=9)\n\nfig.suptitle('EFFECT OF min_samples (eps fixed at 0.5)\\n'\n             'Large dots = Core | Small dots = Border | Red X = Noise',\n             fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig('plot_3_3_min_samples_effect.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 DBSCAN vs k-Means vs Agglomerative on Two Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.4 DBSCAN vs k-Means vs Agglomerative on Two Moons\")\n\nX_moons2, y_moons2 = make_moons(n_samples=200, noise=0.05, random_state=0)\nscaler2 = StandardScaler()\nX_moons2_scaled = scaler2.fit_transform(X_moons2)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nlabels_km = KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X_moons2_scaled)\naxes[0].scatter(X_moons2_scaled[:, 0], X_moons2_scaled[:, 1], c=labels_km, cmap='viridis', s=60)\naxes[0].set_title('k-Means (k=2)\\n\u274c Fails on complex shapes!', fontsize=13)\n\nlabels_agg_m = AgglomerativeClustering(n_clusters=2).fit_predict(X_moons2_scaled)\naxes[1].scatter(X_moons2_scaled[:, 0], X_moons2_scaled[:, 1], c=labels_agg_m, cmap='viridis', s=60)\naxes[1].set_title('Agglomerative (k=2)\\n\u274c Also fails!', fontsize=13)\n\nlabels_db_m = DBSCAN(eps=0.5, min_samples=5).fit_predict(X_moons2_scaled)\naxes[2].scatter(X_moons2_scaled[:, 0], X_moons2_scaled[:, 1], c=labels_db_m, cmap='viridis', s=60)\naxes[2].set_title('DBSCAN (eps=0.5)\\n\u2705 Correctly separates!', fontsize=13)\n\nfor ax in axes:\n    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n\nplt.suptitle('Two Moons: DBSCAN Succeeds Where Others Fail', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.savefig('plot_3_4_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 DBSCAN Core / Border / Noise Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.5 DBSCAN Core / Border / Noise Visualization\")\n\nX_demo, y_demo = make_moons(n_samples=200, noise=0.12, random_state=42)\nscaler_demo = StandardScaler()\nX_demo_scaled = scaler_demo.fit_transform(X_demo)\n\ndb_demo = DBSCAN(eps=0.4, min_samples=5)\nlabels_demo = db_demo.fit_predict(X_demo_scaled)\n\ncore_mask = np.zeros(len(labels_demo), dtype=bool)\ncore_mask[db_demo.core_sample_indices_] = True\nnoise_mask = labels_demo == -1\nborder_mask = (~core_mask) & (~noise_mask)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\nax.scatter(X_demo_scaled[core_mask, 0], X_demo_scaled[core_mask, 1],\n           c=labels_demo[core_mask], cmap='viridis', s=100,\n           edgecolors='black', linewidth=1,\n           label=f'Core Points ({core_mask.sum()})', zorder=3)\n\nax.scatter(X_demo_scaled[border_mask, 0], X_demo_scaled[border_mask, 1],\n           c=labels_demo[border_mask], cmap='viridis', s=50,\n           edgecolors='gray', linewidth=1, marker='s',\n           label=f'Border Points ({border_mask.sum()})', zorder=2)\n\nax.scatter(X_demo_scaled[noise_mask, 0], X_demo_scaled[noise_mask, 1],\n           c='red', s=80, marker='X', linewidth=1,\n           label=f'Noise Points ({noise_mask.sum()})', zorder=4)\n\nfor idx in db_demo.core_sample_indices_[:2]:\n    circle = plt.Circle((X_demo_scaled[idx, 0], X_demo_scaled[idx, 1]),\n                         0.4, fill=False, color='blue', linewidth=1.5,\n                         linestyle='--', alpha=0.5)\n    ax.add_patch(circle)\n\nax.set_xlabel('Feature 0', fontsize=13); ax.set_ylabel('Feature 1', fontsize=13)\nax.set_title('DBSCAN Point Types (eps=0.4, min_samples=5)\\n'\n             'Blue dashed circles = eps neighborhood',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=12, loc='upper right')\nplt.tight_layout()\nplt.savefig('plot_3_5_point_types.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 4. Comparing & Evaluating Clustering (pp. 191\u2013207)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adjusted Rand Index (ARI) \u2014 With Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 65)\nprint(\"4.1 Adjusted Rand Index (ARI)\")\nprint(\"=\" * 65)\n\nX_eval, y_eval = make_moons(n_samples=200, noise=0.05, random_state=0)\nscaler_eval = StandardScaler()\nX_eval_scaled = scaler_eval.fit_transform(X_eval)\n\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X_eval))\n\nalgorithms = {\n    'Random': random_clusters,\n    'k-Means': KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X_eval_scaled),\n    'Agglomerative': AgglomerativeClustering(n_clusters=2).fit_predict(X_eval_scaled),\n    'DBSCAN': DBSCAN().fit_predict(X_eval_scaled),\n}\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 4))\n\nprint('ARI Scores (1.0 = perfect, 0.0 = random):')\nfor ax, (name, labels) in zip(axes, algorithms.items()):\n    ari = adjusted_rand_score(y_eval, labels)\n    print(f'  {name:20s}: {ari:.2f}')\n    ax.scatter(X_eval_scaled[:, 0], X_eval_scaled[:, 1], c=labels, cmap='viridis', s=40)\n    ax.set_title(f'{name}\\nARI: {ari:.2f}', fontsize=12)\n\nplt.suptitle('Adjusted Rand Index (ARI) Comparison', fontsize=15, y=1.05)\nplt.tight_layout()\nplt.savefig('plot_4_1_ari.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Why `accuracy_score` is WRONG for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4.2 Why accuracy_score is WRONG for Clustering\")\n\nclusters1 = [0, 0, 1, 1, 0]\nclusters2 = [1, 1, 0, 0, 1]  # Same grouping, labels just swapped!\n\nprint(f'Clusters1: {clusters1}')\nprint(f'Clusters2: {clusters2}  (identical grouping, different labels)')\nprint(f'\\nAccuracy:  {accuracy_score(clusters1, clusters2):.2f}  <-- WRONG! Says 0%')\nprint(f'ARI:       {adjusted_rand_score(clusters1, clusters2):.2f}  <-- CORRECT! Says 100%')\nprint('Lesson: Cluster labels are arbitrary. Always use ARI or NMI, never accuracy!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Silhouette Score \u2014 Without Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4.3 Silhouette Score (No Ground Truth Needed)\")\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 4))\n\nprint('Silhouette Scores (higher = more compact clusters):')\nfor ax, (name, labels) in zip(axes, algorithms.items()):\n    n_unique = len(set(labels)) - (1 if -1 in labels else 0)\n    sil = silhouette_score(X_eval_scaled, labels) if n_unique >= 2 else -1\n    print(f'  {name:20s}: {sil:.2f}')\n    ax.scatter(X_eval_scaled[:, 0], X_eval_scaled[:, 1], c=labels, cmap='viridis', s=40)\n    ax.set_title(f'{name}\\nSilhouette: {sil:.2f}', fontsize=12)\n\nplt.suptitle('Silhouette Score Comparison', fontsize=15, y=1.05)\nplt.tight_layout()\nplt.savefig('plot_4_3_silhouette.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('\\nNote: k-Means scores HIGHER than DBSCAN even though DBSCAN is visually correct!')\nprint('Silhouette favors compact spherical clusters \u2014 it can be misleading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 5. Real-World Demo \u2014 Iris Dataset (Enhancement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 65)\nprint(\"5. Real-World Demo \u2014 Iris Dataset\")\nprint(\"=\" * 65)\n\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\nprint(f'Iris dataset: {X_iris.shape[0]} samples, {X_iris.shape[1]} features')\nprint(f'Features: {iris.feature_names}')\nprint(f'True classes: {iris.target_names}')\n\nscaler_iris = StandardScaler()\nX_iris_scaled = scaler_iris.fit_transform(X_iris)\npca_iris = PCA(n_components=2)\nX_iris_2d = pca_iris.fit_transform(X_iris_scaled)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\naxes[0, 0].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=y_iris, cmap='viridis', s=50,\n                    edgecolors='black', linewidth=0.5)\naxes[0, 0].set_title('True Labels', fontsize=14)\n\nlabels_km_iris = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_iris_scaled)\nari_km_iris = adjusted_rand_score(y_iris, labels_km_iris)\naxes[0, 1].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=labels_km_iris, cmap='viridis', s=50,\n                    edgecolors='black', linewidth=0.5)\naxes[0, 1].set_title(f'k-Means (k=3) \u2014 ARI: {ari_km_iris:.2f}', fontsize=14)\n\nlabels_agg_iris = AgglomerativeClustering(n_clusters=3).fit_predict(X_iris_scaled)\nari_agg_iris = adjusted_rand_score(y_iris, labels_agg_iris)\naxes[1, 0].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=labels_agg_iris, cmap='viridis', s=50,\n                    edgecolors='black', linewidth=0.5)\naxes[1, 0].set_title(f'Agglomerative \u2014 ARI: {ari_agg_iris:.2f}', fontsize=14)\n\nlabels_db_iris = DBSCAN(eps=0.9, min_samples=5).fit_predict(X_iris_scaled)\nari_db_iris = adjusted_rand_score(y_iris, labels_db_iris)\nn_noise_iris = list(labels_db_iris).count(-1)\naxes[1, 1].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=labels_db_iris, cmap='viridis', s=50,\n                    edgecolors='black', linewidth=0.5)\naxes[1, 1].set_title(f'DBSCAN \u2014 ARI: {ari_db_iris:.2f} ({n_noise_iris} noise pts)', fontsize=14)\n\nfor ax in axes.ravel():\n    ax.set_xlabel('PCA Component 1'); ax.set_ylabel('PCA Component 2')\n\nplt.suptitle('Clustering Algorithms on Iris Dataset', fontsize=16, y=1.01)\nplt.tight_layout()\nplt.savefig('plot_5_iris.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 6. Book's Real-World Example \u2014 Labeled Faces in the Wild (pp. 195\u2013207)\n\n**Dataset:** 2,063 face images, each 87\u00d765 pixels, reduced to 100 dimensions via PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Load and Prepare the Faces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\n\nprint(f'Dataset shape: {people.images.shape}')\nprint(f'Image size: {image_shape}')\nprint(f'Number of people: {len(people.target_names)}')\n\nmask = np.zeros(people.target.shape, dtype=bool)\nfor target in np.unique(people.target):\n    mask[np.where(people.target == target)[0][:50]] = 1\n\nX_people = people.data[mask]\ny_people = people.target[mask]\n\nprint(f'Pixel value range: {X_people.min():.2f} to {X_people.max():.2f}')\nif X_people.max() > 1.0:\n    X_people = X_people / 255.\n    print('Scaled pixels to 0-1 range')\nelse:\n    print('Pixels already in 0-1 range')\n\nprint(f'After balancing: {X_people.shape[0]} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Sample Faces from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle('Sample Faces from the Dataset', fontsize=16, fontweight='bold')\nfor i, (image, label, ax) in enumerate(zip(X_people, y_people, axes.ravel())):\n    ax.imshow(image.reshape(image_shape), cmap='gray')\n    ax.set_title(people.target_names[label].split()[-1], fontsize=11)\nplt.tight_layout()\nplt.savefig('plot_6_2_sample_faces.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 PCA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_faces = PCA(n_components=100, whiten=True, random_state=0)\npca_faces.fit(X_people)\nX_pca = pca_faces.transform(X_people)\n\nprint(f'\\nPCA: {X_people.shape} \u2192 {X_pca.shape}')\nprint(f'Variance explained: {pca_faces.explained_variance_ratio_.sum():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 \u2b50 DBSCAN on Faces \u2014 Outlier Detection (pp. 195\u2013199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- DBSCAN Tuning Process ---')\n\nprint(f'Default (eps=0.5): All noise \u2014 eps too small for 100D data')\nprint(f'min_samples=3 (eps=0.5): Still all noise \u2014 eps is the problem')\n\nprint('\\n--- Exploring different eps values ---')\nfor eps in [1, 3, 5, 7, 9, 11, 13, 15]:\n    dbscan = DBSCAN(eps=eps, min_samples=3)\n    labels = dbscan.fit_predict(X_pca)\n    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise = list(labels).count(-1)\n    sizes = np.bincount(labels + 1)\n    print(f'  eps={eps:>2}: {n_clusters} clusters, {n_noise} noise | sizes: {sizes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 DBSCAN Outlier Faces (eps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_15 = DBSCAN(min_samples=3, eps=15)\nlabels_15 = dbscan_15.fit_predict(X_pca)\n\nnoise_mask_faces = labels_15 == -1\nn_noise_faces = noise_mask_faces.sum()\nprint(f'\\neps=15: {n_noise_faces} noise points (outlier faces)')\n\nnoise_images = X_people[noise_mask_faces]\nn_show = min(n_noise_faces, 15)\ncols = min(n_show, 5)\nrows = max((n_show + cols - 1) // cols, 1)\n\nfig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3.5 * rows),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle('DBSCAN Noise Points \u2014 Outlier Faces (eps=15)',\n             fontsize=15, fontweight='bold', color='red')\n\naxes_flat = axes.ravel() if hasattr(axes, 'ravel') else [axes]\nfor i, ax in enumerate(axes_flat):\n    if i < n_show:\n        ax.imshow(noise_images[i].reshape(image_shape), cmap='gray')\n        ax.set_title(f'Outlier {i+1}', fontsize=10, color='red')\n    else:\n        ax.set_visible(False)\n\nplt.tight_layout()\nplt.savefig('plot_6_5_outlier_faces.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('OUTLIER DETECTION \u2014 a unique strength of DBSCAN!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 DBSCAN eps=7 \u2014 Small Similar Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_7 = DBSCAN(min_samples=3, eps=7)\nlabels_7 = dbscan_7.fit_predict(X_pca)\n\nn_clusters_7 = max(labels_7) + 1\nn_noise_7 = list(labels_7).count(-1)\nprint(f'\\neps=7: {n_clusters_7} clusters, {n_noise_7} noise points')\n\nfor cluster in range(min(n_clusters_7, 10)):\n    mask_c = labels_7 == cluster\n    n_images = np.sum(mask_c)\n    if n_images == 0:\n        continue\n\n    n_show = min(n_images, 8)\n    fig, axes = plt.subplots(1, n_show + 1, figsize=(2.5 * (n_show + 1), 3),\n                             subplot_kw={'xticks': (), 'yticks': ()})\n\n    axes[0].text(0.5, 0.5, f'Cluster {cluster}\\n({n_images} faces)',\n                 ha='center', va='center', fontsize=12, fontweight='bold',\n                 transform=axes[0].transAxes)\n    axes[0].set_frame_on(False)\n\n    images_c = X_people[mask_c]\n    labels_true_c = y_people[mask_c]\n    for i, ax in enumerate(axes[1:]):\n        if i < n_show:\n            ax.imshow(images_c[i].reshape(image_shape), cmap='gray')\n            ax.set_title(people.target_names[labels_true_c[i]].split()[-1], fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\nprint('Each cluster contains genuinely similar faces.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 k-Means on Faces \u2014 Average Faces (pp. 200\u2013202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- k-Means on Faces ---')\n\nkm_faces = KMeans(n_clusters=10, random_state=0, n_init=10)\nlabels_km_faces = km_faces.fit_predict(X_pca)\n\nprint(f'k-Means cluster sizes: {np.bincount(labels_km_faces)}')\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 7),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle('k-Means Cluster Centers \u2014 \"Average Faces\" (k=10)',\n             fontsize=15, fontweight='bold')\n\nfor i, (center, ax) in enumerate(zip(km_faces.cluster_centers_, axes.ravel())):\n    face = pca_faces.inverse_transform(center)\n    ax.imshow(face.reshape(image_shape), cmap='gray')\n    size = np.sum(labels_km_faces == i)\n    ax.set_title(f'Cluster {i} ({size} faces)', fontsize=11)\n\nplt.tight_layout()\nplt.savefig('plot_6_7_kmeans_faces.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Agglomerative on Faces + Dendrogram (pp. 203\u2013207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- Agglomerative on Faces ---')\n\nagg_faces = AgglomerativeClustering(n_clusters=10)\nlabels_agg_faces = agg_faces.fit_predict(X_pca)\n\nprint(f'Agglomerative cluster sizes: {np.bincount(labels_agg_faces)}')\nprint(f'ARI between k-Means and Agglomerative: {adjusted_rand_score(labels_km_faces, labels_agg_faces):.2f}')\n\nlinkage_array = ward(X_pca)\n\nplt.figure(figsize=(20, 6))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel('Sample Index', fontsize=12)\nplt.ylabel('Cluster Distance', fontsize=12)\nplt.title('Dendrogram of Faces Dataset \u2014 No clear natural number of clusters',\n          fontsize=15, fontweight='bold')\nplt.savefig('plot_6_8_faces_dendrogram.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Algorithm Comparison \u2014 Strengths & Weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 75)\nprint('ALGORITHM COMPARISON \u2014 STRENGTHS & WEAKNESSES')\nprint('=' * 75)\nprint(f\"\"\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Feature             \u2502 k-Means      \u2502 Agglomerative  \u2502 DBSCAN       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Must set # clusters \u2502 Yes          \u2502 Yes            \u2502 No  \u2713        \u2502\n\u2502 Complex shapes      \u2502 No           \u2502 No             \u2502 Yes \u2713        \u2502\n\u2502 Noise detection     \u2502 No           \u2502 No             \u2502 Yes \u2713        \u2502\n\u2502 Scalability         \u2502 Excellent \u2713  \u2502 Good           \u2502 Good         \u2502\n\u2502 predict() new data  \u2502 Yes \u2713        \u2502 No             \u2502 No           \u2502\n\u2502 Varying densities   \u2502 Partial      \u2502 Partial        \u2502 Struggles    \u2502\n\u2502 Cluster shapes      \u2502 Spherical    \u2502 Spherical      \u2502 Arbitrary \u2713  \u2502\n\u2502 Key output          \u2502 Centers      \u2502 Dendrogram     \u2502 Core/Border  \u2502\n\u2502 Parameters          \u2502 n_clusters   \u2502 n_clusters,    \u2502 eps,         \u2502\n\u2502                     \u2502              \u2502 linkage        \u2502 min_samples  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\")\n\nprint(\"=\" * 75)\nprint(\"END OF NOTEBOOK\")\nprint(\"=\" * 75)"
   ]
  }
 ]
}