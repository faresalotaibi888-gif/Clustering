{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udd2c Unsupervised Learning \u2014 Clustering (DBSCAN Focus)\n",
    "\n",
    "**Course:** CSC582 \u2014 King Saud University \n",
    "**Reference:** Introduction to Machine Learning with Python \u2014 Chapter 3 (pp. 168\u2013207) \n",
    "**GitHub:** [ClusteringInDBSCAN](https://github.com/YOUR_USERNAME/ClusteringInDBSCAN)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Outline\n",
    "1. **k-Means Clustering** \u2014 basics & failure cases\n",
    "2. **Agglomerative Clustering** \u2014 hierarchical approach & dendrograms\n",
    "3. **DBSCAN** \u2b50 \u2014 density-based clustering (our focus)\n",
    "4. **Comparing & Evaluating** \u2014 ARI, Silhouette Score\n",
    "5. **Real-World Demo** \u2014 Iris dataset with all three algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "# Plot style\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('All imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. k-Means Clustering (pp. 168\u2013181)\n",
    "\n",
    "**How it works:**\n",
    "1. Choose k (number of clusters)\n",
    "2. Randomly initialize k cluster centers\n",
    "3. Assign each point to the nearest center\n",
    "4. Recompute centers as the mean of assigned points\n",
    "5. Repeat 3\u20134 until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data with 3 blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, random_state=1, cluster_std=0.60)\n",
    "\n",
    "# Apply k-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "print(f'Cluster labels (first 20): {kmeans.labels_[:20]}')\n",
    "print(f'Cluster centers:\\n{kmeans.cluster_centers_}')\n",
    "print(f'Inertia (sum of squared distances): {kmeans.inertia_:.2f}')\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=40, alpha=0.7)\n",
    "axes[0].set_title('Original Data (True Labels)', fontsize=14)\n",
    "axes[0].set_xlabel('Feature 0'); axes[0].set_ylabel('Feature 1')\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=40, alpha=0.7)\n",
    "axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                c='red', marker='^', s=200, edgecolors='black', linewidth=2,\n",
    "                label='Cluster Centers')\n",
    "axes[1].set_title('k-Means Clustering (k=3)', fontsize=14)\n",
    "axes[1].set_xlabel('Feature 0'); axes[1].set_ylabel('Feature 1')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Elbow Method \u2014 Choosing Optimal k *(Enhancement)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "sil_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "    sil_scores.append(silhouette_score(X, km.labels_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)'); axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method', fontsize=14)\n",
    "axes[0].axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Optimal k=3')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(K_range, sil_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)'); axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score vs k', fontsize=14)\n",
    "axes[1].axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Optimal k=3')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('The elbow at k=3 confirms the optimal number of clusters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 k-Means Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Case 1: Different densities\n",
    "X_varied, y_varied = make_blobs(n_samples=200, cluster_std=[1.0, 2.5, 0.5], random_state=170)\n",
    "y_pred = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_varied)\n",
    "axes[0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred, cmap='viridis', s=40)\n",
    "axes[0].set_title('Failure: Different Densities', fontsize=13)\n",
    "\n",
    "# Case 2: Non-spherical / elongated clusters\n",
    "X_blob, y_blob = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X_aniso = np.dot(X_blob, transformation)\n",
    "y_pred2 = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_aniso)\n",
    "axes[1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred2, cmap='viridis', s=40)\n",
    "axes[1].set_title('Failure: Non-Spherical Clusters', fontsize=13)\n",
    "\n",
    "# Case 3: Two moons (complex shapes)\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "y_pred3 = KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X_moons)\n",
    "axes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred3, cmap='viridis', s=40)\n",
    "axes[2].set_title('Failure: Complex Shapes (Two Moons)', fontsize=13)\n",
    "\n",
    "plt.suptitle('k-Means Failure Cases', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Agglomerative Clustering (pp. 182\u2013187)\n",
    "\n",
    "**How it works (bottom-up):**\n",
    "1. Each point starts as its own cluster\n",
    "2. Find the two most similar (closest) clusters\n",
    "3. Merge them into one\n",
    "4. Repeat until desired number of clusters reached\n",
    "\n",
    "**Linkage criteria:** Ward (default), Average, Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_agg, y_agg = make_blobs(random_state=1)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X_agg)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_agg[:, 0], X_agg[:, 1], c=assignment, cmap='viridis', s=60,\n",
    "            edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Feature 0'); plt.ylabel('Feature 1')\n",
    "plt.title('Agglomerative Clustering (Ward Linkage, 3 Clusters)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dendro, y_dendro = make_blobs(random_state=0, n_samples=12)\n",
    "linkage_array = ward(X_dendro)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='red', linewidth=2)\n",
    "ax.plot(bounds, [4, 4], '--', c='blue', linewidth=2)\n",
    "ax.text(bounds[1], 7.25, '  2 clusters', va='center', fontsize=13, color='red')\n",
    "ax.text(bounds[1], 4, '  3 clusters', va='center', fontsize=13, color='blue')\n",
    "\n",
    "plt.xlabel('Sample Index'); plt.ylabel('Cluster Distance (Ward)')\n",
    "plt.title('Dendrogram \u2014 Hierarchical Clustering', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \u2b50 3. DBSCAN \u2014 Density-Based Clustering (pp. 187\u2013190)\n",
    "\n",
    "**Key advantages over k-Means and Agglomerative:**\n",
    "- Does NOT require specifying number of clusters\n",
    "- Handles complex, non-convex shapes\n",
    "- Detects noise/outliers automatically\n",
    "\n",
    "**Two parameters:**\n",
    "- `eps` \u2014 radius of neighborhood around each point\n",
    "- `min_samples` \u2014 minimum neighbors to be a core point\n",
    "\n",
    "**Three point types:**\n",
    "- **Core** \u2014 \u2265 min_samples neighbors within eps (heart of cluster)\n",
    "- **Border** \u2014 < min_samples neighbors, but within eps of a core point (edge of cluster)\n",
    "- **Noise** \u2014 not near any core point \u2192 label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 DBSCAN Parameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_db, y_db = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "print('Effect of eps and min_samples on clustering:')\n",
    "print(f'{\"min_samples\":>12} {\"eps\":>6} {\"clusters\":>40}')\n",
    "print('-' * 62)\n",
    "\n",
    "for min_s in [2, 3, 5]:\n",
    "    for eps_val in [1.0, 1.5, 2.0, 3.0]:\n",
    "        db = DBSCAN(min_samples=min_s, eps=eps_val)\n",
    "        clusters = db.fit_predict(X_db)\n",
    "        print(f'{min_s:>12} {eps_val:>6.1f} {str(clusters):>40}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Effect of `eps` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.06, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "eps_values = [0.1, 0.2, 0.3, 0.5, 0.8, 1.5]\n",
    "\n",
    "for ax, eps_val in zip(axes.ravel(), eps_values):\n",
    "    db = DBSCAN(eps=eps_val, min_samples=5)\n",
    "    labels = db.fit_predict(X_scaled)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    # Noise in red\n",
    "    noise_mask = labels == -1\n",
    "    ax.scatter(X_scaled[noise_mask, 0], X_scaled[noise_mask, 1],\n",
    "               c='red', marker='x', s=50, label=f'Noise ({n_noise} pts)', zorder=3)\n",
    "    # Cluster points\n",
    "    cluster_mask = labels != -1\n",
    "    if cluster_mask.any():\n",
    "        ax.scatter(X_scaled[cluster_mask, 0], X_scaled[cluster_mask, 1],\n",
    "                   c=labels[cluster_mask], cmap='viridis', s=40,\n",
    "                   edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    # Show eps radius circle\n",
    "    if eps_val <= 0.5:\n",
    "        circle = plt.Circle((X_scaled[150, 0], X_scaled[150, 1]),\n",
    "                           eps_val, fill=False, color='red', linewidth=2,\n",
    "                           linestyle='--', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.plot(X_scaled[150, 0], X_scaled[150, 1], 'r*', markersize=15, zorder=5)\n",
    "    \n",
    "    if n_clusters == 0: result, color = 'ALL NOISE!', 'red'\n",
    "    elif n_clusters == 2: result, color = 'CORRECT!', 'green'\n",
    "    elif n_clusters == 1: result, color = 'One big cluster', 'orange'\n",
    "    else: result, color = 'Too fragmented', 'orange'\n",
    "    \n",
    "    ax.set_title(f'eps = {eps_val}\\n{n_clusters} clusters, {n_noise} noise \u2014 {result}',\n",
    "                fontsize=13, fontweight='bold', color=color)\n",
    "    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "fig.suptitle('EFFECT OF eps (min_samples fixed at 5)\\n'\n",
    "             'Red dashed circle = eps neighborhood radius',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Effect of `min_samples` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "min_samples_values = [2, 3, 5, 10, 20, 50]\n",
    "\n",
    "for ax, min_s in zip(axes.ravel(), min_samples_values):\n",
    "    db = DBSCAN(eps=0.5, min_samples=min_s)\n",
    "    labels = db.fit_predict(X_scaled)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    # Identify point types\n",
    "    core_mask = np.zeros(len(labels), dtype=bool)\n",
    "    if hasattr(db, 'core_sample_indices_') and len(db.core_sample_indices_) > 0:\n",
    "        core_mask[db.core_sample_indices_] = True\n",
    "    \n",
    "    noise_mask = labels == -1\n",
    "    border_mask = (labels != -1) & (~core_mask)\n",
    "    \n",
    "    # Plot noise\n",
    "    ax.scatter(X_scaled[noise_mask, 0], X_scaled[noise_mask, 1],\n",
    "               c='red', marker='x', s=50, label=f'Noise ({n_noise})', zorder=3)\n",
    "    # Plot border (small)\n",
    "    if border_mask.any():\n",
    "        ax.scatter(X_scaled[border_mask, 0], X_scaled[border_mask, 1],\n",
    "                   c=labels[border_mask], cmap='viridis', s=30,\n",
    "                   edgecolors='black', linewidth=0.3, alpha=0.6)\n",
    "    # Plot core (large)\n",
    "    if core_mask.any():\n",
    "        ax.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n",
    "                   c=labels[core_mask], cmap='viridis', s=60,\n",
    "                   edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    n_core = core_mask.sum()\n",
    "    n_border = border_mask.sum()\n",
    "    ax.set_title(f'min_samples = {min_s}\\n{n_clusters} clusters | '\n",
    "                f'Core: {n_core} | Border: {n_border} | Noise: {n_noise}',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "fig.suptitle('EFFECT OF min_samples (eps fixed at 0.5)\\n'\n",
    "             'Large dots = Core | Small dots = Border | Red X = Noise',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 DBSCAN vs k-Means vs Agglomerative on Two Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# k-Means (fails)\n",
    "labels_km = KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X_moons_scaled)\n",
    "axes[0].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=labels_km, cmap='viridis', s=60)\n",
    "axes[0].set_title('k-Means (k=2)\\n\u274c Fails on complex shapes!', fontsize=13)\n",
    "\n",
    "# Agglomerative (fails)\n",
    "labels_agg = AgglomerativeClustering(n_clusters=2).fit_predict(X_moons_scaled)\n",
    "axes[1].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=labels_agg, cmap='viridis', s=60)\n",
    "axes[1].set_title('Agglomerative (k=2)\\n\u274c Also fails!', fontsize=13)\n",
    "\n",
    "# DBSCAN (succeeds!)\n",
    "labels_db = DBSCAN(eps=0.5, min_samples=5).fit_predict(X_moons_scaled)\n",
    "axes[2].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=labels_db, cmap='viridis', s=60)\n",
    "axes[2].set_title('DBSCAN (eps=0.5)\\n\u2705 Correctly separates!', fontsize=13)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Feature 0'); ax.set_ylabel('Feature 1')\n",
    "\n",
    "plt.suptitle('Two Moons: DBSCAN Succeeds Where Others Fail', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 DBSCAN Core / Border / Noise Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with visible noise points\n",
    "X_demo, y_demo = make_moons(n_samples=200, noise=0.12, random_state=42)\n",
    "scaler_demo = StandardScaler()\n",
    "X_demo_scaled = scaler_demo.fit_transform(X_demo)\n",
    "\n",
    "db_demo = DBSCAN(eps=0.4, min_samples=5)\n",
    "labels_demo = db_demo.fit_predict(X_demo_scaled)\n",
    "\n",
    "core_mask = np.zeros(len(labels_demo), dtype=bool)\n",
    "core_mask[db_demo.core_sample_indices_] = True\n",
    "noise_mask = labels_demo == -1\n",
    "border_mask = (~core_mask) & (~noise_mask)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Core points (large circles)\n",
    "ax.scatter(X_demo_scaled[core_mask, 0], X_demo_scaled[core_mask, 1],\n",
    "           c=labels_demo[core_mask], cmap='viridis', s=100,\n",
    "           edgecolors='black', linewidth=1,\n",
    "           label=f'Core Points ({core_mask.sum()})', zorder=3)\n",
    "\n",
    "# Border points (small squares)\n",
    "ax.scatter(X_demo_scaled[border_mask, 0], X_demo_scaled[border_mask, 1],\n",
    "           c=labels_demo[border_mask], cmap='viridis', s=50,\n",
    "           edgecolors='gray', linewidth=1, marker='s',\n",
    "           label=f'Border Points ({border_mask.sum()})', zorder=2)\n",
    "\n",
    "# Noise points (red X)\n",
    "ax.scatter(X_demo_scaled[noise_mask, 0], X_demo_scaled[noise_mask, 1],\n",
    "           c='red', s=80, marker='X', linewidth=1,\n",
    "           label=f'Noise Points ({noise_mask.sum()})', zorder=4)\n",
    "\n",
    "# Draw eps circles around two core points\n",
    "for idx in db_demo.core_sample_indices_[:2]:\n",
    "    circle = plt.Circle((X_demo_scaled[idx, 0], X_demo_scaled[idx, 1]),\n",
    "                       0.4, fill=False, color='blue', linewidth=1.5,\n",
    "                       linestyle='--', alpha=0.5)\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.set_xlabel('Feature 0', fontsize=13); ax.set_ylabel('Feature 1', fontsize=13)\n",
    "ax.set_title('DBSCAN Point Types (eps=0.4, min_samples=5)\\n'\n",
    "             'Blue dashed circles = eps neighborhood',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comparing & Evaluating Clustering (pp. 191\u2013207)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adjusted Rand Index (ARI) \u2014 With Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval, y_eval = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_eval_scaled = scaler.fit_transform(X_eval)\n",
    "\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X_eval))\n",
    "\n",
    "algorithms = {\n",
    "    'Random': random_clusters,\n",
    "    'k-Means': KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X_eval_scaled),\n",
    "    'Agglomerative': AgglomerativeClustering(n_clusters=2).fit_predict(X_eval_scaled),\n",
    "    'DBSCAN': DBSCAN().fit_predict(X_eval_scaled),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "print('ARI Scores (1.0 = perfect, 0.0 = random):')\n",
    "for ax, (name, labels) in zip(axes, algorithms.items()):\n",
    "    ari = adjusted_rand_score(y_eval, labels)\n",
    "    print(f'  {name:20s}: {ari:.2f}')\n",
    "    ax.scatter(X_eval_scaled[:, 0], X_eval_scaled[:, 1], c=labels, cmap='viridis', s=40)\n",
    "    ax.set_title(f'{name}\\nARI: {ari:.2f}', fontsize=12)\n",
    "\n",
    "plt.suptitle('Adjusted Rand Index (ARI) Comparison', fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Why accuracy_score is WRONG for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters1 = [0, 0, 1, 1, 0]\n",
    "clusters2 = [1, 1, 0, 0, 1]  # Same grouping, labels just swapped!\n",
    "\n",
    "print(f'Clusters1: {clusters1}')\n",
    "print(f'Clusters2: {clusters2}  (identical grouping, different labels)')\n",
    "print(f'\\nAccuracy:  {accuracy_score(clusters1, clusters2):.2f}  <-- WRONG! Says 0%')\n",
    "print(f'ARI:       {adjusted_rand_score(clusters1, clusters2):.2f}  <-- CORRECT! Says 100%')\n",
    "print('\\nLesson: Cluster labels are arbitrary. Always use ARI or NMI, never accuracy!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Silhouette Score \u2014 Without Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "print('Silhouette Scores (higher = more compact clusters):')\n",
    "for ax, (name, labels) in zip(axes, algorithms.items()):\n",
    "    n_unique = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    sil = silhouette_score(X_eval_scaled, labels) if n_unique >= 2 else -1\n",
    "    print(f'  {name:20s}: {sil:.2f}')\n",
    "    ax.scatter(X_eval_scaled[:, 0], X_eval_scaled[:, 1], c=labels, cmap='viridis', s=40)\n",
    "    ax.set_title(f'{name}\\nSilhouette: {sil:.2f}', fontsize=12)\n",
    "\n",
    "plt.suptitle('Silhouette Score Comparison', fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('\\nNote: k-Means scores HIGHER than DBSCAN even though DBSCAN is visually correct!')\n",
    "print('Silhouette favors compact spherical clusters \u2014 it can be misleading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Real-World Demo \u2014 Iris Dataset *(Enhancement)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(f'Iris dataset: {X_iris.shape[0]} samples, {X_iris.shape[1]} features')\n",
    "print(f'Features: {iris.feature_names}')\n",
    "print(f'True classes: {iris.target_names}')\n",
    "\n",
    "# Scale and reduce to 2D for visualization\n",
    "scaler_iris = StandardScaler()\n",
    "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
    "pca_iris = PCA(n_components=2)\n",
    "X_iris_2d = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# True labels\n",
    "axes[0, 0].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=y_iris, cmap='viridis', s=50,\n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "axes[0, 0].set_title('True Labels', fontsize=14)\n",
    "\n",
    "# k-Means\n",
    "labels_km = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_iris_scaled)\n",
    "ari_km = adjusted_rand_score(y_iris, labels_km)\n",
    "axes[0, 1].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=labels_km, cmap='viridis', s=50,\n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].set_title(f'k-Means (k=3) \u2014 ARI: {ari_km:.2f}', fontsize=14)\n",
    "\n",
    "# Agglomerative\n",
    "labels_agg = AgglomerativeClustering(n_clusters=3).fit_predict(X_iris_scaled)\n",
    "ari_agg = adjusted_rand_score(y_iris, labels_agg)\n",
    "axes[1, 0].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=labels_agg, cmap='viridis', s=50,\n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "axes[1, 0].set_title(f'Agglomerative \u2014 ARI: {ari_agg:.2f}', fontsize=14)\n",
    "\n",
    "# DBSCAN\n",
    "labels_db = DBSCAN(eps=0.9, min_samples=5).fit_predict(X_iris_scaled)\n",
    "ari_db = adjusted_rand_score(y_iris, labels_db)\n",
    "n_noise = list(labels_db).count(-1)\n",
    "axes[1, 1].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=labels_db, cmap='viridis', s=50,\n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "axes[1, 1].set_title(f'DBSCAN \u2014 ARI: {ari_db:.2f} ({n_noise} noise pts)', fontsize=14)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.set_xlabel('PCA Component 1'); ax.set_ylabel('PCA Component 2')\n",
    "\n",
    "plt.suptitle('Clustering Algorithms on Iris Dataset', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary \u2014 Algorithm Comparison\n",
    "\n",
    "| Feature | k-Means | Agglomerative | DBSCAN |\n",
    "|---------|---------|---------------|--------|\n",
    "| Must set # clusters | Yes | Yes | **No** |\n",
    "| Complex shapes | No | No | **Yes** |\n",
    "| Noise detection | No | No | **Yes** |\n",
    "| Scalability | Excellent | Good | Good |\n",
    "| Cluster sizes | Even | Even | Varies |\n",
    "| Interpretability | Cluster centers | Dendrogram | Core/border/noise |\n",
    "| Key parameters | n_clusters | n_clusters, linkage | eps, min_samples |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Book's Real-World Example \u2014 Labeled Faces in the Wild (pp. 195\u2013207)\n",
    "\n",
    "This section replicates the book's analysis of the **Labeled Faces in the Wild (LFW)** dataset.\n",
    "It demonstrates how each algorithm behaves on real, high-dimensional image data.\n",
    "\n",
    "**Dataset:** 2,063 face images, each 87\u00d765 pixels, reduced to 100 dimensions via PCA.\n",
    "\n",
    "**Key findings from the book:**\n",
    "- DBSCAN reveals outlier faces (odd angles, hands covering faces, wearing hats)\n",
    "- k-Means produces smooth \"average face\" cluster centers\n",
    "- Agglomerative clustering with 40 clusters finds meaningful face groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load and Prepare the Faces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Load faces \u2014 this may take a minute to download the first time\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "image_shape = people.images[0].shape\n",
    "\n",
    "print(f'Dataset shape: {people.images.shape}')\n",
    "print(f'Image size: {image_shape}')\n",
    "print(f'Number of people: {len(people.target_names)}')\n",
    "\n",
    "# Limit to 50 images per person (to balance the dataset)\n",
    "mask = np.zeros(people.target.shape, dtype=bool)\n",
    "for target in np.unique(people.target):\n",
    "    mask[np.where(people.target == target)[0][:50]] = 1\n",
    "\n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]\n",
    "\n",
    "# Scale pixel values to 0-1\n",
    "X_people = X_people / 255.\n",
    "\n",
    "print(f'\\nAfter balancing: {X_people.shape[0]} images')\n",
    "print(f'Features per image: {X_people.shape[1]} (pixels)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Sample Faces from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample faces\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "fig.suptitle('Sample Faces from the Dataset', fontsize=16, fontweight='bold')\n",
    "for i, (image, label, ax) in enumerate(zip(X_people, y_people, axes.ravel())):\n",
    "    ax.imshow(image.reshape(image_shape), cmap='gray')\n",
    "    ax.set_title(people.target_names[label].split()[-1], fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 PCA Preprocessing\n",
    "\n",
    "Working with raw pixels (3,000+ features) is impractical.\n",
    "We use PCA to reduce to 100 meaningful components \u2014 this is what the book does on p.195."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with PCA (as the book does)\n",
    "pca = PCA(n_components=100, whiten=True, random_state=0)\n",
    "pca.fit(X_people)\n",
    "X_pca = pca.transform(X_people)\n",
    "\n",
    "print(f'Original shape: {X_people.shape}')\n",
    "print(f'After PCA:      {X_pca.shape}')\n",
    "print(f'Variance explained: {pca.explained_variance_ratio_.sum():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 \u2b50 DBSCAN on Faces \u2014 Outlier Detection (Book pp. 195\u2013199)\n",
    "\n",
    "The book shows how DBSCAN behaves very differently from the other algorithms:\n",
    "- Default parameters \u2192 everything is noise\n",
    "- With tuned eps \u2192 one big cluster + noise (outlier faces)\n",
    "- The \"noise\" faces are genuinely unusual: hats, odd angles, hands in front of faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Default DBSCAN \u2014 everything is noise!\n",
    "print('=== DBSCAN Tuning Process ===')\n",
    "print()\n",
    "\n",
    "dbscan_default = DBSCAN()\n",
    "labels_default = dbscan_default.fit_predict(X_pca)\n",
    "print(f'Default (eps=0.5): Unique labels = {np.unique(labels_default)}')\n",
    "print('  \u2192 All points are noise! eps is too small for 100D data.')\n",
    "\n",
    "# Step 2: Try min_samples=3\n",
    "dbscan_ms3 = DBSCAN(min_samples=3)\n",
    "labels_ms3 = dbscan_ms3.fit_predict(X_pca)\n",
    "print(f'\\nmin_samples=3 (eps=0.5): Unique labels = {np.unique(labels_ms3)}')\n",
    "print('  \u2192 Still all noise. eps is the real problem.')\n",
    "\n",
    "# Step 3: Increase eps\n",
    "print('\\n--- Exploring different eps values ---')\n",
    "for eps in [1, 3, 5, 7, 9, 11, 13, 15]:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_pca)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    sizes = np.bincount(labels + 1)  # +1 because noise is -1\n",
    "    print(f'  eps={eps:>2}: {n_clusters} clusters, {n_noise} noise | sizes: {sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps=15 gives 1 big cluster + 27 noise points \u2014 let's see the noise faces!\n",
    "dbscan_15 = DBSCAN(min_samples=3, eps=15)\n",
    "labels_15 = dbscan_15.fit_predict(X_pca)\n",
    "\n",
    "noise_mask = labels_15 == -1\n",
    "n_noise = noise_mask.sum()\n",
    "print(f'eps=15: {n_noise} noise points (outlier faces)')\n",
    "print(f'These are the \"unusual\" faces that DBSCAN flagged as not fitting any group.\\n')\n",
    "\n",
    "# Show the noise (outlier) faces\n",
    "noise_images = X_people[noise_mask]\n",
    "n_show = min(n_noise, 15)\n",
    "cols = min(n_show, 5)\n",
    "rows = (n_show + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3.5 * rows),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "fig.suptitle('DBSCAN Noise Points \u2014 Outlier Faces (eps=15)',\n",
    "             fontsize=15, fontweight='bold', color='red')\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i < n_show:\n",
    "        ax.imshow(noise_images[i].reshape(image_shape), cmap='gray')\n",
    "        ax.set_title(f'Outlier {i+1}', fontsize=10, color='red')\n",
    "    else:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Notice: odd angles, hats, hands in front of faces, unusual crops.')\n",
    "print('This is OUTLIER DETECTION \u2014 a unique strength of DBSCAN!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 DBSCAN eps=7 \u2014 Finding Small Similar Clusters (Book p. 198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps=7 produces the most interesting result: many small clusters\n",
    "dbscan_7 = DBSCAN(min_samples=3, eps=7)\n",
    "labels_7 = dbscan_7.fit_predict(X_pca)\n",
    "\n",
    "n_clusters = max(labels_7) + 1\n",
    "n_noise = list(labels_7).count(-1)\n",
    "print(f'eps=7: {n_clusters} clusters, {n_noise} noise points\\n')\n",
    "\n",
    "# Show each small cluster\n",
    "for cluster in range(min(n_clusters, 10)):  # Show up to 10 clusters\n",
    "    mask = labels_7 == cluster\n",
    "    n_images = np.sum(mask)\n",
    "    if n_images == 0:\n",
    "        continue\n",
    "    \n",
    "    n_show = min(n_images, 8)\n",
    "    fig, axes = plt.subplots(1, n_show + 1, figsize=(2.5 * (n_show + 1), 3),\n",
    "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    \n",
    "    # Label\n",
    "    axes[0].text(0.5, 0.5, f'Cluster {cluster}\\n({n_images} faces)',\n",
    "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "                transform=axes[0].transAxes)\n",
    "    axes[0].set_frame_on(False)\n",
    "    \n",
    "    images = X_people[mask]\n",
    "    labels_true = y_people[mask]\n",
    "    for i, ax in enumerate(axes[1:]):\n",
    "        if i < n_show:\n",
    "            ax.imshow(images[i].reshape(image_shape), cmap='gray')\n",
    "            ax.set_title(people.target_names[labels_true[i]].split()[-1], fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('\\nNotice: each cluster contains faces with similar orientation,')\n",
    "print('expression, or the same person. DBSCAN finds genuine visual similarity!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 k-Means on Faces \u2014 Cluster Centers as \"Average Faces\" (Book pp. 200\u2013202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means with 10 clusters\n",
    "km_faces = KMeans(n_clusters=10, random_state=0, n_init=10)\n",
    "labels_km_faces = km_faces.fit_predict(X_pca)\n",
    "\n",
    "print(f'k-Means cluster sizes: {np.bincount(labels_km_faces)}')\n",
    "\n",
    "# Show cluster centers as \"average faces\"\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "fig.suptitle('k-Means Cluster Centers \u2014 \"Average Faces\" (k=10)',\n",
    "             fontsize=15, fontweight='bold')\n",
    "\n",
    "for i, (center, ax) in enumerate(zip(km_faces.cluster_centers_, axes.ravel())):\n",
    "    # Transform cluster center back to image space\n",
    "    face = pca.inverse_transform(center)\n",
    "    ax.imshow(face.reshape(image_shape), cmap='gray', vmin=0, vmax=1)\n",
    "    size = np.sum(labels_km_faces == i)\n",
    "    ax.set_title(f'Cluster {i} ({size} faces)', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Each center is a smooth \"average\" of all faces in that cluster.')\n",
    "print('Notice different orientations, expressions, and lighting conditions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster: show center + 5 closest + 5 furthest faces\n",
    "print('For each cluster: Center \u2192 5 closest faces \u2192 5 furthest faces')\n",
    "print('(Closest = most typical, Furthest = least typical)\\n')\n",
    "\n",
    "for cluster_idx in range(10):\n",
    "    mask = labels_km_faces == cluster_idx\n",
    "    cluster_points = X_pca[mask]\n",
    "    cluster_images = X_people[mask]\n",
    "    \n",
    "    # Distances to cluster center\n",
    "    center = km_faces.cluster_centers_[cluster_idx]\n",
    "    dists = np.sqrt(np.sum((cluster_points - center) ** 2, axis=1))\n",
    "    \n",
    "    closest = np.argsort(dists)[:5]\n",
    "    furthest = np.argsort(dists)[-5:]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 11, figsize=(22, 2.5),\n",
    "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    \n",
    "    # Center\n",
    "    axes[0].imshow(pca.inverse_transform(center).reshape(image_shape),\n",
    "                   cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0].set_title(f'Center {cluster_idx}', fontsize=9, color='blue', fontweight='bold')\n",
    "    \n",
    "    # 5 closest\n",
    "    for i, idx in enumerate(closest):\n",
    "        axes[1 + i].imshow(cluster_images[idx].reshape(image_shape), cmap='gray')\n",
    "        axes[1 + i].set_title('closest', fontsize=8, color='green')\n",
    "    \n",
    "    # 5 furthest\n",
    "    for i, idx in enumerate(furthest):\n",
    "        axes[6 + i].imshow(cluster_images[idx].reshape(image_shape), cmap='gray')\n",
    "        axes[6 + i].set_title('furthest', fontsize=8, color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Agglomerative Clustering on Faces \u2014 Dendrogram (Book pp. 203\u2013207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative with 10 clusters\n",
    "agg_faces = AgglomerativeClustering(n_clusters=10)\n",
    "labels_agg_faces = agg_faces.fit_predict(X_pca)\n",
    "\n",
    "print(f'Agglomerative cluster sizes: {np.bincount(labels_agg_faces)}')\n",
    "print(f'\\nCompare with k-Means sizes: {np.bincount(labels_km_faces)}')\n",
    "\n",
    "# ARI between k-Means and Agglomerative\n",
    "ari_km_agg = adjusted_rand_score(labels_km_faces, labels_agg_faces)\n",
    "print(f'\\nARI between k-Means and Agglomerative: {ari_km_agg:.2f}')\n",
    "print('Low ARI means they found quite different groupings!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram of the faces dataset\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "linkage_array = ward(X_pca)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\n",
    "plt.xlabel('Sample Index', fontsize=12)\n",
    "plt.ylabel('Cluster Distance', fontsize=12)\n",
    "plt.title('Dendrogram of Faces Dataset \u2014 No clear natural number of clusters',\n",
    "          fontsize=15, fontweight='bold')\n",
    "plt.show()\n",
    "print('Unlike the toy datasets, there is no obvious \"cut point\" here.')\n",
    "print('This confirms DBSCAN\\'s finding: faces don\\'t form distinct groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample faces from each agglomerative cluster\n",
    "print('Sample faces from each Agglomerative cluster (10 clusters):\\n')\n",
    "\n",
    "for cluster in range(10):\n",
    "    mask = labels_agg_faces == cluster\n",
    "    n_images = np.sum(mask)\n",
    "    n_show = min(n_images, 8)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_show + 1, figsize=(2.5 * (n_show + 1), 3),\n",
    "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    \n",
    "    axes[0].text(0.5, 0.5, f'Cluster {cluster}\\n({n_images} faces)',\n",
    "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "                transform=axes[0].transAxes)\n",
    "    axes[0].set_frame_on(False)\n",
    "    \n",
    "    images = X_people[mask]\n",
    "    labels_true = y_people[mask]\n",
    "    for i, ax in enumerate(axes[1:]):\n",
    "        if i < n_show:\n",
    "            ax.imshow(images[i].reshape(image_shape), cmap='gray')\n",
    "            ax.set_title(people.target_names[labels_true[i]].split()[-1], fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Final Comparison \u2014 All Three Algorithms on Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 65)\n",
    "print('SUMMARY: Clustering Algorithms on Faces Dataset')\n",
    "print('=' * 65)\n",
    "print()\n",
    "print(f'{\"Algorithm\":<25} {\"Clusters\":<12} {\"Noise\":<10} {\"Key Finding\"}')\n",
    "print('-' * 65)\n",
    "print(f'{\"DBSCAN (eps=15)\":<25} {\"1\":<12} {noise_mask.sum():<10} Outlier detection \u2014 unusual faces flagged')\n",
    "print(f'{\"DBSCAN (eps=7)\":<25} {n_clusters:<12} {list(labels_7).count(-1):<10} Small clusters of very similar faces')\n",
    "print(f'{\"k-Means (k=10)\":<25} {\"10\":<12} {\"0\":<10} Smooth average-face centers')\n",
    "print(f'{\"Agglomerative (k=10)\":<25} {\"10\":<12} {\"0\":<10} Uneven sizes, different from k-Means')\n",
    "print()\n",
    "print('Key insight: Face images don\\'t form distinct natural groups.')\n",
    "print('DBSCAN revealed this honestly (1 big cluster + outliers),')\n",
    "print('while k-Means and Agglomerative forced even partitions.')"
   ]
  }
 ]
}